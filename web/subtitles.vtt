WEBVTT FILE

1
00:00:05.000 --> 00:00:15.000
So when we do a column organization, then
we come instantly to the fact that we can

2
00:00:15.000 --> 00:00:27.000
see how to minimize the access to memory.
This can be done first by only accessing the

3
00:00:27.000 --> 00:00:36.000
columns by need and second when we look into
columns we can do something with them that

4
00:00:36.000 --> 00:00:45.000
we couldn’t do with a normal disk-based
database, we couldn’t do an attribute-based

5
00:00:45.000 --> 00:00:58.000
compression in a row database on disk, but
we can do a column-based compression in memory.

6
00:00:58.000 --> 00:01:06.000
This is what we will look at in one example
now and then you get a feeling of what we

7
00:01:06.000 --> 00:01:14.000
can achieve and why this column concept becomes
superior.

8
00:01:14.000 --> 00:01:20.000
So we learned already that main memory access
is the bottleneck and we want to reduce the

9
00:01:20.000 --> 00:01:31.000
overall main memory access and one way is
to reduce the bits we need to store the information

10
00:01:31.000 --> 00:01:38.000
which is in a table.

11
00:01:38.000 --> 00:01:45.000
So we want to introduce data compression and
we want to do this in a way that the operations

12
00:01:45.000 --> 00:01:54.000
that we do on a table can be done in a compressed
form. For the ones who know row-based compression

13
00:01:54.000 --> 00:02:04.000
- at least the one SAP has used in the last
forty years - do not allow execution in a

14
00:02:04.000 --> 00:02:11.000
compressed form. So before anything could
happen on an attribute in tuple, the tuple

15
00:02:11.000 --> 00:02:14.000
had to be decompressed.

16
00:02:14.000 --> 00:02:30.000
Our example is the world population. 8 billion
humans, probably we have 200 Bytes per person,

17
00:02:30.000 --> 00:02:40.000
and in the old days I had the number of bytes
needed by multiplying 8 billion by 200, 1.6

18
00:02:40.000 --> 00:02:50.000
TB. Now we want to see how this comes down
when we start working on the data.

19
00:02:50.000 --> 00:02:56.000
We have here our sample table. Each tuple
has a record ID [recID]. This is the relative

20
00:02:56.000 --> 00:03:19.000
position in the table. And then we have the
attributes. Now we start a concept which is

21
00:03:19.000 --> 00:03:24.000
called Dictionary Encoding.

22
00:03:24.000 --> 00:03:34.000
We take now an individual column. Let’s
take the first name column, and reorganize

23
00:03:34.000 --> 00:03:47.000
the data in a vector which is pointing into
a dictionary. In there we keep the names,

24
00:03:47.000 --> 00:03:58.000
the actual instance, John, Mary, Jane, Peter,
and in this dictionary we have a relative

25
00:03:58.000 --> 00:04:07.000
position of the name which is an integer,
and use the integer then in the actual column.

26
00:04:07.000 --> 00:04:16.000
So we replace the character string “John”
by an integer pointing to a character string.

27
00:04:16.000 --> 00:04:30.000
So we have 8 billion people with only 5 million
first names. So for these we need only 23

28
00:04:30.000 --> 00:04:41.000
bits to encode them. So we do a little design
assessment here - how many bytes do we need

29
00:04:41.000 --> 00:04:58.000
for first names? There are countries where
first names are pretty extended, we have nobody

30
00:04:58.000 --> 00:05:08.000
from India here, but there are others in the
world. So let us take 23 bits instead of 20

31
00:05:08.000 --> 00:05:25.000
bytes
and a dictionary. So we get all the distinct
values of the first names into a dictionary,

32
00:05:25.000 --> 00:05:33.000
we sort the dictionary, then every name has
a position: John is on position 23, Mary is

33
00:05:33.000 --> 00:05:45.000
on 24. This is an integer of the length of
23 bits and we store this integer value, called

34
00:05:45.000 --> 00:05:56.000
valueID, in the attribute vector for first
name. The compression is now 23 bits, probably

35
00:05:56.000 --> 00:06:04.000
we take 24 bits, this is three bytes and we
had 20 bytes before, so we have a compression

36
00:06:04.000 --> 00:06:22.000
from twenty to three plus the dictionary.
So we saved 17 bytes and times 8 billion,

37
00:06:22.000 --> 00:06:32.000
this is quite a lot. Always remember if we
reduce that amount of space we need to store

38
00:06:32.000 --> 00:06:39.000
the information, we increase the speed of
looking through all these information. In

39
00:06:39.000 --> 00:06:58.000
this case we increase the scan of first names
already by roughly a factor seven. Without

40
00:06:58.000 --> 00:07:07.000
doing anything the full column scan speed
is now seven times faster than in a database

41
00:07:07.000 --> 00:07:19.000
which stores the characters.
When we do this with gender, then it becomes

42
00:07:19.000 --> 00:07:29.000
even more obvious. For gender we need only
one bit instead of “m” and “f”, so

43
00:07:29.000 --> 00:07:44.000
we compress now from eight bits to one, factor
eight. We have a bit vector which is 8 billion

44
00:07:44.000 --> 00:08:01.000
bits long, that is one billion characters,
and we want to scan through the vector male/female,

45
00:08:01.000 --> 00:08:09.000
we have got to check for bit on and off, then
we can achieve a very fast scan speed.

46
00:08:09.000 --> 00:08:19.000
So with the dictionary there is a design decision
very early on, to keep the dictionary sorted.

47
00:08:19.000 --> 00:08:26.000
This is good for Inserts, this is good for
Joins. If the dictionaries are sorted we can

48
00:08:26.000 --> 00:08:39.000
do binary search on top of the dictionaries,
so we can find the distinct value in log n.

49
00:08:39.000 --> 00:08:49.000
We need this for the speed of transferring
incoming character-based tuples into dictionary-encoded

50
00:08:49.000 --> 00:08:57.000
tuples.
The dictionary entries themselves can be compressed

51
00:08:57.000 --> 00:09:11.000
and we will see at least one compression technique
later. The compression rate depends on the

52
00:09:11.000 --> 00:09:21.000
cardinality of a column. How many different
values are in a column? If we have only two

53
00:09:21.000 --> 00:09:30.000
different values, like male and female, in
a column, then I can compress this down to

54
00:09:30.000 --> 00:09:40.000
one bit, and if I use the byte as before this
is eight to one; in the case of first names

55
00:09:40.000 --> 00:09:47.000
probably like seven to one.

56
00:09:47.000 --> 00:09:54.000
There are two different definitions here for
cardinality. When we talk about the cardinality

57
00:09:54.000 --> 00:10:09.000
of a table then it is just the number

58
00:10:09.000 --> 00:10:21.000
of tuples in the relation, if we don’t have
duplicates in it. A column cardinality is

59
00:10:21.000 --> 00:10:28.000
defined as the number of distinct values in
a column, so the cardinality of the gender

60
00:10:28.000 --> 00:10:39.000
column is very low. The column with the highest
cardinality here - that’s a trick question.

61
00:10:39.000 --> 00:10:50.000
Countries - it’s only 200, currencies is
even less. Cities is a good one - one million

62
00:10:50.000 --> 00:11:03.000
only. So you get a feeling how the actual
character description of real world objects

63
00:11:03.000 --> 00:11:13.000
are abstract. The cardinality of those objects
is relatively small and when we use this for

64
00:11:13.000 --> 00:11:21.000
encoding we can achieve significant compression
rates.

65
00:11:21.000 --> 00:11:28.000
By the way this was done in the old days in
building Enterprise Systems a lot, when you

66
00:11:28.000 --> 00:11:39.000
look at an older SAP system there is a lot
of coding, so there is not a country text

67
00:11:39.000 --> 00:11:45.000
in a customer record, there is a country code
pointing to a country. So this type of dictionary

68
00:11:45.000 --> 00:11:53.000
encoding was introduced lots of years ago
in Enterprise Computing, sometimes painful,

69
00:11:53.000 --> 00:12:02.000
because you always have to translate it back
and if you want to show where 007 lives, you

70
00:12:02.000 --> 00:12:08.000
have to go into 007’s country code, and
from there into the country table, and then

71
00:12:08.000 --> 00:12:13.000
you find out he comes from Great Britain.
Systems which were developed later very often

72
00:12:13.000 --> 00:12:20.000
didn’t do this, systems which came from
the PC world mainly worked on the original

73
00:12:20.000 --> 00:12:32.000
value strings, so the country is a field and
there is a country coded. You might have the

74
00:12:32.000 --> 00:12:39.000
same country several times misspelled, so
there are some kinds of application issues.

75
00:12:39.000 --> 00:12:48.000
What we do here is we automatically code all
columns and compress them as much as we can

76
00:12:48.000 --> 00:12:59.000
on bits. The nice thing is we don’t have
to work on byte level anymore, because with

77
00:12:59.000 --> 00:13:06.000
the modern Intel CPUs - and I think others
are following - we have got more instructions

78
00:13:06.000 --> 00:13:14.000
where we can shift the 64 bits back and forth
and we can do all kinds of bit oriented operations

79
00:13:14.000 --> 00:13:18.000
at a very high speed.
So we have a sorted dictionary, we have this,

80
00:13:18.000 --> 00:13:26.000
and we have a compression rate. So the first
name is five million, the last name is eight

81
00:13:26.000 --> 00:13:34.000
million, the gender two, city one million,
country 200, birthday 40,000. So just to get

82
00:13:34.000 --> 00:13:46.000
a feeling, we have the entropy and we have
an item size, we even have here 49 bytes,

83
00:13:46.000 --> 00:13:55.000
so I should have looked to this first. With
a dictionary we can compress this significantly.

84
00:13:55.000 --> 00:14:02.000
Here are the compression rates between plain
and dictionary compression on the right hand

85
00:14:02.000 --> 00:14:08.000
side.
When we look at the compression, at this point

86
00:14:08.000 --> 00:14:16.000
we can already say that in large enterprise
tables coming from an SAP system - I have

87
00:14:16.000 --> 00:14:27.000
in mind now a table with 500 attributes from
sales - and we have learned already, that

88
00:14:27.000 --> 00:14:34.000
probably only 20 percent of the attributes
have been used in one single installation.

89
00:14:34.000 --> 00:14:48.000
That means that 400 fields are not populated,
400 columns are all empty. What does a database

90
00:14:48.000 --> 00:14:54.000
do with a column that is empty? There is no
column. So the first compression we get is

91
00:14:54.000 --> 00:15:04.000
that empty columns are not being stored. They
are known to the system in the meta data,

92
00:15:04.000 --> 00:15:16.000
there is probably a little management information
that the column is empty, and we can use that

93
00:15:16.000 --> 00:15:23.000
in the system. Definitely it doesn’t cost
space and definitely we do not need to search

94
00:15:23.000 --> 00:15:35.000
columns which are not populated. The second
is, we compress the remaining columns and

95
00:15:35.000 --> 00:15:45.000
the compression rate we see here, if the average
compression is factor five this is probably

96
00:15:45.000 --> 00:15:55.000
rule of thumb. So compression to the factor
five of populated data and compression of

97
00:15:55.000 --> 00:16:05.000
close to 100 percent for non-populated data.
If you found out the amount of population

98
00:16:05.000 --> 00:16:14.000
for a given table - that is just a little
program running and should be a feature of

99
00:16:14.000 --> 00:16:24.000
the system management tools, I think it is
- then we can already predict the compression

100
00:16:24.000 --> 00:16:33.000
before we ever do something with a table.
So knowing the distinct values and the size

101
00:16:33.000 --> 00:16:38.000
of the table, we can calculate the populated
ones and the unpopulated ones you don’t

102
00:16:38.000 --> 00:16:47.000
have to calculate. If we want to compare with
the plain table, how it probably comes into

103
00:16:47.000 --> 00:16:54.000
the system, so all fields populated, even
with the default values, we have the compression

104
00:16:54.000 --> 00:17:01.000
factor. This is various from five, which is
typically minimum - these are tables where

105
00:17:01.000 --> 00:17:10.000
all attributes are populated, to factor 50
- these are tables in SAP financials - and

106
00:17:10.000 --> 00:17:20.000
it varies in between. Taking any type of table,
companies have set up then factor ten is probably

107
00:17:20.000 --> 00:17:30.000
a very good compression factor, so if somebody
says “we have today a ten TB table space

108
00:17:30.000 --> 00:17:43.000
and we do not compress the data - this is
for example in MaxDB or in DB2 - then we need

109
00:17:43.000 --> 00:19:43.000
only one TB in this database.

